#!/usr/bin/env python3
"""
DMVA Calibrated Performance Calculator

Based on actual execution results from DMVA migrations.
Calibrated against real Teradata -> Snowflake migration timings.

Reference data:
- DERV_ACCT_PATY: 7 min total (5+2+2+1 phases)
- ACCT_BASE: 6 min total (2+1+2+2 phases)
- PLAN_BALN_SEGM_MSTR: 22 min total (20+6+10+3 phases, with max_select_columns=154)
"""

import math
from dataclasses import dataclass
from typing import Optional, Dict, List

@dataclass
class TableProfile:
    """Table characteristics for estimation"""
    name: str
    size_gb: float
    row_count: int
    column_count: int
    partition_count: int = 1
    
    # Aggregation settings
    distinct_count: bool = True
    null_count: bool = False
    min_values: bool = False
    max_values: bool = False
    sum_values: bool = False
    
    # Configuration
    max_select_columns: int = 1000


@dataclass
class MigrationEstimate:
    """Estimated migration times and resources"""
    table_name: str
    
    # Phase timings (minutes)
    td_checksum_measure_mins: float
    td_unload_mins: float
    sf_load_mins: float
    sf_store_checksum_mins: float
    total_mins: float
    
    # Query details
    num_measure_queries: int
    aggregates_per_query: int
    
    # Resource requirements
    disk_space_gb: float
    
    # Analysis
    bottleneck: str
    warnings: List[str]
    recommendations: List[str]


class DMVACalibratedCalculator:
    """
    Calibrated calculator based on actual DMVA execution data.
    
    Performance model derived from real migrations:
    - Checksum/Measure: ~0.015 min per column per measure query + base overhead
    - Unload: ~0.01 min per GB (Teradata throughput: 100 MB/s)
    - Load: ~0.02 min per GB (Snowflake Small WH)
    - Store Checksum: ~0.5 min base + 0.01 min per partition
    
    Total time accounts for pipeline parallelism between phases.
    """
    
    def __init__(self):
        # Calibrated coefficients from actual DMVA execution data
        # Based on: PLAN_BALN_SEGM_MSTR (20+6+10+3 = 22 total)
        
        # Checksum/Measure phase is complex: involves checksumming + profiling aggregations
        self.checksum_base_mins = 2.0  # Base overhead (connection, setup, etc.)
        self.checksum_per_query_mins = 3.0  # Actual Teradata query execution time per measure query
        self.checksum_per_column_factor = 0.08  # Time scales with column count (complex aggregations)
        
        # Unload phase: Extract data from Teradata
        # Actual: 60GB in ~6 mins = 10 GB/min throughput
        self.unload_mins_per_gb = 0.1  # Conservative Teradata unload: ~10 GB/min
        self.unload_base_mins = 0.5  # Connection and setup overhead
        
        # Load phase: Copy into Snowflake
        # Actual: 60GB in ~10 mins = 6 GB/min (Small warehouse)
        self.load_mins_per_gb = 0.17  # Snowflake Small WH: ~6 GB/min
        self.load_base_mins = 1.0  # Parse and validation overhead
        
        # Store checksum phase: Insert validation results into DMVA tables
        self.store_checksum_base_mins = 1.0  # Base time for validation storage
        self.store_checksum_per_partition_mins = 0.4  # Time per partition validation
        
        # Pipeline parallelism factor
        # Actual data: 20+6+10+3=39 sequential, but actual=22, so 22/39 = 0.56 efficiency
        self.pipeline_efficiency = 0.55  # 55% - significant overlap between unload/load phases
        
        # Teradata aggregate limit
        self.teradata_aggregate_limit = 77
        self.teradata_aggregate_warning = 64
    
    def calculate_measure_queries(self, column_count: int, num_aggregations: int, 
                                  max_select_columns: int) -> tuple[int, int, int]:
        """
        Calculate number of measure queries needed.
        
        Based on system.py logic:
        measure_column_count = 1 (column_name) + num_aggregations
        columns_per_query = floor(max_select_columns / measure_column_count)
        num_queries = ceil(column_count / columns_per_query)
        
        Returns: (num_queries, columns_per_query, aggregates_per_query)
        """
        measure_items_per_column = 1 + num_aggregations
        columns_per_query = max(math.floor(max_select_columns / measure_items_per_column), 1)
        num_queries = math.ceil(column_count / columns_per_query)
        aggregates_per_query = min(column_count, columns_per_query) * num_aggregations
        
        return num_queries, columns_per_query, aggregates_per_query
    
    def estimate_checksum_measure_time(self, table: TableProfile) -> tuple[float, int, int]:
        """Estimate TD checksum and measure time."""
        num_aggregations = sum([
            table.distinct_count,
            table.null_count,
            table.min_values,
            table.max_values,
            table.sum_values
        ])
        
        num_queries, cols_per_query, aggs_per_query = self.calculate_measure_queries(
            table.column_count, num_aggregations, table.max_select_columns
        )
        
        # Time = base + (queries √ó time_per_query) + (columns √ó complexity_factor)
        time_mins = (
            self.checksum_base_mins +
            (num_queries * self.checksum_per_query_mins) +
            (table.column_count * num_aggregations * self.checksum_per_column_factor)
        )
        
        # Note: Measure queries run once per table, not per partition
        # Partitions affect unload/load phases but not the measure phase
        
        return time_mins, num_queries, aggs_per_query
    
    def estimate_unload_time(self, table: TableProfile) -> float:
        """Estimate TD unload time."""
        time_mins = self.unload_base_mins + (table.size_gb * self.unload_mins_per_gb)
        
        # Parallel unload for multiple partitions (up to 4x speedup)
        if table.partition_count > 1:
            parallel_factor = min(table.partition_count, 4)
            time_mins = time_mins / math.sqrt(parallel_factor)
        
        return time_mins
    
    def estimate_load_time(self, table: TableProfile) -> float:
        """Estimate SF load time."""
        time_mins = self.load_base_mins + (table.size_gb * self.load_mins_per_gb)
        return time_mins
    
    def estimate_store_checksum_time(self, table: TableProfile) -> float:
        """Estimate SF checksum storage and validation time."""
        time_mins = (
            self.store_checksum_base_mins +
            (table.partition_count * self.store_checksum_per_partition_mins)
        )
        return time_mins
    
    def calculate_total_time(self, checksum_mins: float, unload_mins: float,
                            load_mins: float, store_mins: float) -> float:
        """
        Calculate total time accounting for pipeline parallelism.
        
        DMVA can overlap phases:
        - While loading one partition, unload the next
        - Checksum storage happens post-load per partition
        
        Model: total ‚âà checksum + max(unload, load) + store √ó efficiency_factor
        """
        # Checksum must complete before unload starts
        # Unload and load can overlap across partitions
        # Store happens after load
        
        data_movement_time = max(unload_mins, load_mins)
        sequential_time = checksum_mins + data_movement_time + store_mins
        
        # Apply pipeline efficiency (actual data shows ~60% of sequential time)
        total_time = checksum_mins + (data_movement_time + store_mins) * self.pipeline_efficiency
        
        return round(total_time, 1)
    
    def identify_bottleneck(self, checksum_mins: float, unload_mins: float,
                           load_mins: float, store_mins: float) -> str:
        """Identify the performance bottleneck."""
        max_time = max(checksum_mins, unload_mins, load_mins, store_mins)
        
        if max_time == checksum_mins:
            return "TD Checksum/Measure"
        elif max_time == unload_mins:
            return "TD Unload"
        elif max_time == load_mins:
            return "SF Load"
        else:
            return "SF Store Checksum"
    
    def generate_warnings(self, table: TableProfile, aggregates_per_query: int) -> List[str]:
        """Generate warnings based on configuration."""
        warnings = []
        
        if aggregates_per_query > self.teradata_aggregate_limit:
            warnings.append(
                f"‚ùå CRITICAL: {aggregates_per_query} aggregates per query exceeds "
                f"Teradata limit ({self.teradata_aggregate_limit}). Migration will FAIL!"
            )
        elif aggregates_per_query > self.teradata_aggregate_warning:
            warnings.append(
                f"‚ö†Ô∏è  WARNING: {aggregates_per_query} aggregates per query is near "
                f"Teradata limit. Risk of failure!"
            )
        
        if table.size_gb > 100 and table.partition_count <= 1:
            warnings.append(
                f"‚ÑπÔ∏è  Large table ({table.size_gb} GB) with only 1 partition. "
                "Consider partitioning for better performance."
            )
        
        return warnings
    
    def generate_recommendations(self, table: TableProfile, aggregates_per_query: int,
                                 bottleneck: str) -> List[str]:
        """Generate optimization recommendations."""
        recommendations = []
        
        # Fix aggregate limit issues
        if aggregates_per_query > self.teradata_aggregate_warning:
            num_aggregations = sum([
                table.distinct_count, table.null_count, table.min_values,
                table.max_values, table.sum_values
            ])
            measure_items = 1 + num_aggregations
            recommended_max = math.floor(self.teradata_aggregate_warning * measure_items / num_aggregations)
            recommendations.append(
                f"Set max_select_columns = {recommended_max} in overrides"
            )
        
        # Optimize bottleneck
        if bottleneck == "TD Checksum/Measure":
            if table.distinct_count and table.null_count and table.min_values and table.max_values:
                recommendations.append(
                    "Consider disabling some aggregations (null_count, min_values, max_values) "
                    "if not needed"
                )
        elif bottleneck == "SF Load":
            recommendations.append("Consider using a Medium or Large Snowflake warehouse")
        elif bottleneck == "TD Unload":
            recommendations.append("Check Teradata resource availability and consider partitioning")
        
        return recommendations
    
    def estimate(self, table: TableProfile) -> MigrationEstimate:
        """Generate complete migration estimate for a table."""
        
        # Calculate each phase
        checksum_mins, num_queries, aggs_per_query = self.estimate_checksum_measure_time(table)
        unload_mins = self.estimate_unload_time(table)
        load_mins = self.estimate_load_time(table)
        store_mins = self.estimate_store_checksum_time(table)
        
        # Calculate total with pipeline efficiency
        total_mins = self.calculate_total_time(checksum_mins, unload_mins, load_mins, store_mins)
        
        # Identify bottleneck
        bottleneck = self.identify_bottleneck(checksum_mins, unload_mins, load_mins, store_mins)
        
        # Generate warnings and recommendations
        warnings = self.generate_warnings(table, aggs_per_query)
        recommendations = self.generate_recommendations(table, aggs_per_query, bottleneck)
        
        # Disk space (compressed parquet + safety factor)
        disk_space_gb = math.ceil(table.size_gb * 0.6 * 2)
        
        return MigrationEstimate(
            table_name=table.name,
            td_checksum_measure_mins=round(checksum_mins, 1),
            td_unload_mins=round(unload_mins, 1),
            sf_load_mins=round(load_mins, 1),
            sf_store_checksum_mins=round(store_mins, 1),
            total_mins=total_mins,
            num_measure_queries=num_queries,
            aggregates_per_query=aggs_per_query,
            disk_space_gb=disk_space_gb,
            bottleneck=bottleneck,
            warnings=warnings,
            recommendations=recommendations
        )
    
    def print_estimate(self, estimate: MigrationEstimate):
        """Print formatted estimate."""
        print("\n" + "=" * 80)
        print(f"MIGRATION ESTIMATE: {estimate.table_name}")
        print("=" * 80)
        
        print("\nüìä Phase Breakdown:")
        print(f"   TD - Checksum/Measure:  {estimate.td_checksum_measure_mins:6.1f} mins")
        print(f"   TD - Unload:            {estimate.td_unload_mins:6.1f} mins")
        print(f"   SF - Load:              {estimate.sf_load_mins:6.1f} mins")
        print(f"   SF - Store Checksum:    {estimate.sf_store_checksum_mins:6.1f} mins")
        print(f"   {'‚îÄ' * 40}")
        print(f"   Total (with overlap):   {estimate.total_mins:6.1f} mins ({estimate.total_mins/60:.2f} hrs)")
        
        print(f"\nüéØ Configuration:")
        print(f"   Measure queries:        {estimate.num_measure_queries}")
        print(f"   Aggregates per query:   {estimate.aggregates_per_query}")
        print(f"   Disk space needed:      {estimate.disk_space_gb} GB")
        
        print(f"\nüîç Analysis:")
        print(f"   Bottleneck:             {estimate.bottleneck}")
        
        if estimate.warnings:
            print(f"\n‚ö†Ô∏è  Warnings:")
            for warning in estimate.warnings:
                print(f"   {warning}")
        
        if estimate.recommendations:
            print(f"\nüí° Recommendations:")
            for rec in estimate.recommendations:
                print(f"   ‚Ä¢ {rec}")
        
        if not estimate.warnings:
            print(f"\n‚úÖ Configuration looks good!")
        
        print("\n" + "=" * 80)


def validate_against_actual_data():
    """Validate calculator against actual execution results."""
    print("\n" + "=" * 80)
    print("VALIDATION AGAINST ACTUAL DATA")
    print("=" * 80)
    
    calc = DMVACalibratedCalculator()
    
    # Actual result 3: PLAN_BALN_SEGM_MSTR
    # The only one with detailed specs from the note
    print("\nTable: PLAN_BALN_SEGM_MSTR")
    print("Actual results: TD=20, Unload=6, Load=10, Store=3, Total=22 mins")
    print("Config: max_select_columns=154, distinct_count=true, others=false")
    
    # Need to estimate table size and columns - let's reverse engineer
    # If unload = 6 mins and rate = 0.01 min/GB, size ‚âà 600 GB seems too high
    # More likely: 60 GB table (6 mins @ 10 GB/min unload)
    # Load = 10 mins @ 6 GB/min = 60 GB ‚úì
    # Checksum = 20 mins suggests many columns with distinct_count
    
    table = TableProfile(
        name="PLAN_BALN_SEGM_MSTR",
        size_gb=60,
        row_count=50_000_000,  # Estimated
        column_count=100,  # Estimated based on 20 min checksum time
        partition_count=5,  # Estimated
        distinct_count=True,
        null_count=False,
        min_values=False,
        max_values=False,
        sum_values=False,
        max_select_columns=154
    )
    
    estimate = calc.estimate(table)
    calc.print_estimate(estimate)
    
    print(f"\nüìà Comparison:")
    print(f"   Actual Total:    22 mins")
    print(f"   Estimated Total: {estimate.total_mins} mins")
    print(f"   Accuracy:        {100 - abs(estimate.total_mins - 22)/22*100:.1f}%")


def main():
    """Example usage with different scenarios."""
    calc = DMVACalibratedCalculator()
    
    # Scenario 1: Your 135-column table with problem
    print("\n" + "=" * 80)
    print("SCENARIO 1: 135-Column Table (Current Configuration)")
    print("=" * 80)
    
    table1 = TableProfile(
        name="YOUR_135_COLUMN_TABLE",
        size_gb=100,
        row_count=10_000_000,
        column_count=135,
        partition_count=5,
        distinct_count=True,
        null_count=False,
        min_values=False,
        max_values=False,
        sum_values=False,
        max_select_columns=1000  # Current setting
    )
    
    estimate1 = calc.estimate(table1)
    calc.print_estimate(estimate1)
    
    # Scenario 2: Same table with fix
    print("\n" + "=" * 80)
    print("SCENARIO 2: 135-Column Table (Fixed Configuration)")
    print("=" * 80)
    
    table2 = TableProfile(
        name="YOUR_135_COLUMN_TABLE",
        size_gb=100,
        row_count=10_000_000,
        column_count=135,
        partition_count=5,
        distinct_count=True,
        null_count=False,
        min_values=False,
        max_values=False,
        sum_values=False,
        max_select_columns=76  # Fixed setting
    )
    
    estimate2 = calc.estimate(table2)
    calc.print_estimate(estimate2)
    
    # Validation against actual data
    validate_against_actual_data()


if __name__ == "__main__":
    main()
